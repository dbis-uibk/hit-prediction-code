{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:\n",
    "Data preprocessing is a predominant step in machine learning to yield highly accurate and insightful results. Greater the quality of data, greater is the reliance on the produced results. **Incomplete, noisy, and inconsistent data** are the properties of large real-world datasets. Data preprocessing helps in increasing the quality of data by filling in missing incomplete data, smoothing noise and resolving inconsistencies.\n",
    "\n",
    "* **Incomplete data** can occur for a number of reasons. Attributes of interest may not always be available, such as customer information for sales transaction data. Relevant data may not be recorded due to a misunderstanding, or because of equipment malfunctions.\n",
    "* There are many possible reasons for **noisy data** (having incorrect attribute values). The data collection instruments used may be faulty. There may have been human or computer errors occurring at data entry. Errors in data transmission can also occur. Incorrect data may also result from inconsistencies in naming conventions or data codes used, or inconsistent formats for input fields, such as date.\n",
    "\n",
    "There are a number of data preprocessing techniques available such as,\n",
    "1. **Data Cleaning**\n",
    "2. **Data Integration**\n",
    "3. **Data Transformation**\n",
    "4. **Data Reduction**\n",
    "\n",
    "* **Data cleaning** can be applied to filling in missing values, remove noise, resolving inconsistencies, identifying and removing outliers in the data. \n",
    "* **Data integration** merges data from multiple sources into a coherent data store, such as a data warehouse. \n",
    "* **Data transformations**, such as normalization, may be applied. For example, normalization may improve the accuracy and efficiency of mining algorithms involving distance measurements. \n",
    "* **Data reduction** can reduce the data size by eliminating redundant features, or clustering, for instance. \n",
    "\n",
    "**Reference**: Data Mining:Concepts and Techniques Second Edition, Jiawei Han, Micheline Kamber.\n",
    "\n",
    "**PS:** This is my first kaggle notebook contribution. Hope you like it!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import (GradientBoostingRegressor, GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet('../data/hit_song_prediction_ismir2020/processed/msd_bb_mbid_cleaned_matches_ab_unique.parquet')\n",
    "dataset['weeks'] = dataset['weeks'].fillna(0)\n",
    "dataset['peakPos'] = dataset['peakPos'].fillna(150)\n",
    "dataset['hit'] = (dataset['peakPos'] < 100.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning\n",
    "\n",
    "Select used columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hit_prediction_code.common as common\n",
    "\n",
    "data = dataset[common.get_columns_matching_list(dataset.columns, common.all_no_year_list()).union(['uuid'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Find the missing percentage of each columns in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_percent(data):\n",
    "    \"\"\"\n",
    "    Returns dataframe containing the total missing values and percentage of total\n",
    "    missing values of a column.\n",
    "    \"\"\"\n",
    "    miss_df = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for col in data.columns:\n",
    "        sum_miss_val = data[col].isnull().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_df = miss_df.append(dict(zip(miss_df.columns,[col,sum_miss_val,percent_miss_val])),ignore_index=True)\n",
    "    return miss_df\n",
    "\n",
    "miss_df = find_missing_percent(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Displays columns with missing values'''\n",
    "missing = 0.0\n",
    "\n",
    "missing_df = miss_df[miss_df['PercentMissing']>missing].sort_values(by='PercentMissing', ascending=False)\n",
    "display(missing_df)\n",
    "print(\"\\n\")\n",
    "print(f\"Number of columns with more than {missing}% missing values:{str(missing_df.shape[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Segregate the numeric and categoric data'''\n",
    "numeric_cols = data.select_dtypes(['float', 'int']).columns\n",
    "categoric_cols = data.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, col1, col2, last_one=False):\n",
    "    \"\"\"\n",
    "    Plot the histogram for the numerical columns.\n",
    "    \n",
    "    Freedman-Diaconis Rule:\n",
    "    Freedman-Diaconis Rule is a rule to find the optimal number of bins.\n",
    "    Bin width: (2 * IQR)/(N^1/3)\n",
    "    N - Size of the data\n",
    "    Number of bins : (Range/ bin-width)\n",
    "    \n",
    "    Disadvantage: The IQR might be zero for certain columns. In\n",
    "    that case the bin width might be equal to infinity. In that case \n",
    "    the actual range of the data is returned as bin width.\n",
    "    \n",
    "    Sturges Rule:\n",
    "    Sturges Rule is a rule to find the optimal number of bins.\n",
    "    Bin width: (Range/ bin-width)\n",
    "    N - Size of the data\n",
    "    Number of bins : ceil(log2(N))+1\n",
    "    \n",
    "    \"\"\"\n",
    "    number_of_bins = 40\n",
    "    \n",
    "    freq1, bin_edges1 = np.histogram(data[col1], bins=number_of_bins)\n",
    "    freq2, bin_edges2 = np.histogram(data[col2], bins=number_of_bins)\n",
    "        \n",
    "    if(last_one!=True):\n",
    "        plt.figure(figsize=(45,18))  \n",
    "        ax1 = plt.subplot(1,2,1)\n",
    "        ax1.set_title(col1,fontsize=45)\n",
    "        ax1.set_xlabel(col1,fontsize=40)\n",
    "        ax1.set_ylabel('Frequency',fontsize=40)\n",
    "        data[col1].hist(bins=bin_edges1,ax = ax1, xlabelsize=30, ylabelsize=30)   \n",
    "    else:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        ax1 = plt.subplot(1,2,1)\n",
    "        ax1.set_title(col1,fontsize=25)\n",
    "        ax1.set_xlabel(col1,fontsize=20)\n",
    "        ax1.set_ylabel('Frequency',fontsize=20)\n",
    "        data[col1].hist(bins=bin_edges1,ax = ax1, xlabelsize=15, ylabelsize=15)\n",
    "    \n",
    "    if(last_one != True):\n",
    "        ax2 = plt.subplot(1,2,2)\n",
    "        ax2.set_title(col2,fontsize=45)\n",
    "        ax2.set_xlabel(col2,fontsize=40)\n",
    "        ax2.set_ylabel('Frequency',fontsize=40)\n",
    "        data[col2].hist(bins=bin_edges2, ax = ax2, xlabelsize=30, ylabelsize=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_cols = list(filter(lambda c: c.startswith('highlevel'), numeric_cols))\n",
    "# hist_cols = data.columns\n",
    "\n",
    "hist_cols = sorted(hist_cols)\n",
    "for i in range(0,len(hist_cols),2):\n",
    "    if(i == len(hist_cols)-1):\n",
    "        plot_histogram(data, hist_cols[i], hist_cols[i], True)\n",
    "    else:\n",
    "        plot_histogram(data, hist_cols[i], hist_cols[i+1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strange_data = ((data['highlevel.gender.all.male'] > 0.377) & (data['highlevel.gender.all.male'] < 0.378)) | ((data['highlevel.mood_relaxed.all.relaxed'] > 0.808) & (data['highlevel.mood_relaxed.all.relaxed'] < 0.809))\n",
    "strange_data = data['highlevel.mood_electronic.all.electronic'] > 0.979\n",
    "\n",
    "strange_info = dataset.merge(data[strange_data], on=['uuid'])\n",
    "display(strange_info['highlevel.mood_electronic.all.electronic_x'].describe())\n",
    "\n",
    "display(strange_info['weeks'].plot.hist())\n",
    "electronic = list(filter(lambda c: c.startswith('metadata.version'), strange_info.columns))\n",
    "display(strange_info[['lastfm_listener_count', 'lastfm_playcount', 'weeks', 'peakPos', 'highlevel.mood_electronic.all.electronic_x'] + electronic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data[~strange_data]\n",
    "\n",
    "final_data = dataset.merge(clean_data, on=['uuid'])[['weeks', 'peakPos']]\n",
    "\n",
    "        \n",
    "# display(final_data['hit'].plot.hist())\n",
    "\n",
    "for i in range(0,len(hist_cols),2):\n",
    "    if(i == len(hist_cols)-1):\n",
    "        plot_histogram(clean_data, hist_cols[i], hist_cols[i], True)\n",
    "    else:\n",
    "        plot_histogram(clean_data, hist_cols[i], hist_cols[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Transformation\n",
    "### 3.1 Skewed data:\n",
    "![](https://miro.medium.com/max/1200/1*nj-Ch3AUFmkd0JUSOW_bTQ.jpeg)\n",
    "\n",
    "* If skewness is less than -1 or greater than 1, the distribution is **highly skewed**.\n",
    "* If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is **moderately skewed**.\n",
    "* If skewness is between -0.5 and 0.5, the distribution is **approximately symmetric**.\n",
    "* If skewness is 0 the distribution is **symmetric**\n",
    "\n",
    "#### 3.1.1 **Positively skewed data:**\n",
    "* **Log transformation** (when the data is highly skewed)\n",
    "    * log(X) - if no zero values are present\n",
    "    * log(C + X) - if zero values are present\n",
    "        * C is a constant added so that the smallest value will be equal to 1.\n",
    "* **Square root transformation** (when the data is moderately skewed)\n",
    "    * sqrt(X)\n",
    "    \n",
    "#### 3.1.2 **Negatively skewed data:**\n",
    "* Reflect and Log transformation\n",
    "    * log(K - X) - K is a constant from which the values are subtracted so that the smallest value is 1.\n",
    "    * (K - X) makes the large number small and the small number large so the negatively skewed data becomes positively skewed.\n",
    "* Reflect and Square root transformation\n",
    "    * sqrt(K - X) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_skewness(data, numeric_cols):\n",
    "    \"\"\"\n",
    "    Calculate the skewness of the columns and segregate the positive\n",
    "    and negative skewed data.\n",
    "    \"\"\"\n",
    "    skew_dict = {}\n",
    "    for col in numeric_cols:\n",
    "        skew_dict[col] = data[col].skew()\n",
    "\n",
    "    skew_dict = dict(sorted(skew_dict.items(),key=itemgetter(1)))\n",
    "    positive_skew_dict = {k:v for (k,v) in skew_dict.items() if v>0}\n",
    "    negative_skew_dict = {k:v for (k,v) in skew_dict.items() if v<0}\n",
    "    return skew_dict, positive_skew_dict, negative_skew_dict\n",
    "\n",
    "def add_constant(data, highly_pos_skewed):\n",
    "    \"\"\"\n",
    "    Look for zeros in the columns. If zeros are present then the log(0) would result in -infinity.\n",
    "    So before transforming it we need to add it with some constant.\n",
    "    \"\"\"\n",
    "    C = 1\n",
    "    for col in highly_pos_skewed.keys():\n",
    "        if(col != 'SalePrice'):\n",
    "            if(len(data[data[col] == 0]) > 0):\n",
    "                data[col] = data[col] + C\n",
    "    return data\n",
    "\n",
    "def log_transform(data, highly_pos_skewed):\n",
    "    \"\"\"\n",
    "    Log transformation of highly positively skewed columns.\n",
    "    \"\"\"\n",
    "    for col in highly_pos_skewed.keys():\n",
    "        if(col != 'SalePrice'):\n",
    "            data[col] = np.log10(data[col])\n",
    "    return data\n",
    "\n",
    "def sqrt_transform(data, moderately_pos_skewed):\n",
    "    \"\"\"\n",
    "    Square root transformation of moderately skewed columns.\n",
    "    \"\"\"\n",
    "    for col in moderately_pos_skewed.keys():\n",
    "        if(col != 'SalePrice'):\n",
    "            data[col] = np.sqrt(data[col])\n",
    "    return data\n",
    "\n",
    "def reflect_sqrt_transform(data, moderately_neg_skewed):\n",
    "    \"\"\"\n",
    "    Reflection and log transformation of highly negatively skewed \n",
    "    columns.\n",
    "    \"\"\"\n",
    "    for col in moderately_neg_skewed.keys():\n",
    "        if(col != 'SalePrice'):\n",
    "            K = max(data[col]) + 1\n",
    "            data[col] = np.sqrt(K - data[col])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n",
    "If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n",
    "If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n",
    "\"\"\"\n",
    "skew_dict, positive_skew_dict, negative_skew_dict = find_skewness(data, numeric_cols)\n",
    "moderately_pos_skewed = {k:v for (k,v) in positive_skew_dict.items() if v>0.5 and v<=1}\n",
    "highly_pos_skewed = {k:v for (k,v) in positive_skew_dict.items() if v>1}\n",
    "moderately_neg_skewed = {k:v for (k,v) in negative_skew_dict.items() if v>-1 and v<=0.5}\n",
    "highly_neg_skewed = {k:v for (k,v) in negative_skew_dict.items() if v<-1}\n",
    "\n",
    "display(highly_pos_skewed, moderately_pos_skewed, moderately_neg_skewed)\n",
    "\n",
    "'''Transform data.'''\n",
    "# data = add_constant(data, highly_pos_skewed)\n",
    "# data = log_transform(data, highly_pos_skewed)\n",
    "# data = sqrt_transform(data, moderately_pos_skewed)\n",
    "# data = reflect_sqrt_transform(data, moderately_neg_skewed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
