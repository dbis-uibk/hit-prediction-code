{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d51981a",
   "metadata": {},
   "source": [
    "# Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c284394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from hit_prediction_code.analytics import get_results_as_dataframe\n",
    "from hit_prediction_code import analytics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "results = get_results_as_dataframe(\n",
    "    project_name='hit-prediction-ismir2020',\n",
    "    table_name='hit_prediction',\n",
    "    filter_git_dirty=True,\n",
    "#     date_filter='BETWEEN \\'2021-04-07 14:00:00\\' AND \\'2021-04-13 17:00:00\\'',\n",
    "#     date_filter='>= \\'2022-03-24 14:21:00\\'',\n",
    "    date_filter='>= \\'2022-04-07 10:35:00\\'',\n",
    "    columns=['id', 'sourcefile', 'outcome'],\n",
    "#     filters=['sourcefile LIKE \\'plans/wide_and_deep_performance/%%ordinal_one_hot_all_lfmlc.py\\''],\n",
    "    filters=['sourcefile LIKE \\'plans/hsppw/qcut_hsp-s%%.py\\''],\n",
    "#     filters=['sourcefile LIKE \\'plans/ordinal%%\\''],\n",
    ")\n",
    "\n",
    "analytics.add_approach_to_df(results)\n",
    "analytics.add_cv_epoch_evaluator_outcome_to_df(results)\n",
    "\n",
    "results.sort_values(by='sourcefile', inplace=True)\n",
    "display(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae_from_cm(matrix, average='macro'):\n",
    "    assert matrix.shape[0] == matrix.shape[1]\n",
    "       \n",
    "    if average == 'macro':\n",
    "        distance = matrix * _distance_multiplyer(matrix.shape[0])\n",
    "        \n",
    "        class_distance = np.sum(distance, axis=1)\n",
    "        class_counts = np.sum(matrix, axis=1)\n",
    "        \n",
    "        mask = class_counts > 0\n",
    "        \n",
    "        return np.mean(class_distance[mask] / class_counts[mask])\n",
    "\n",
    "def _distance_multiplyer(n):\n",
    "    m = []\n",
    "    for i in range(n):\n",
    "        r = range(-i, n-i, 1)\n",
    "        m.append(r)\n",
    "    \n",
    "    return abs(np.array(m))\n",
    "\n",
    "def compute_kappa_from_cm(matrix):\n",
    "    cidot = matrix.sum(axis=1)\n",
    "    cdotj = matrix.sum(axis=0)\n",
    "    N = matrix.sum()\n",
    "    \n",
    "    def w(i, j):\n",
    "        return abs(i - j)\n",
    "    \n",
    "    def e(i, j):\n",
    "        return cidot[i] * cdotj[j] / N\n",
    "        \n",
    "    def c(i, j):\n",
    "        return matrix[i, j]\n",
    "    \n",
    "    nom = 0\n",
    "    dis = 0\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            nom += (w(i,j) * c(i, j))\n",
    "            dis += (w(i,j) * e(i, j))\n",
    "    \n",
    "    return 1 - nom / dis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1396de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_names(num_splits):\n",
    "    return ['split-' + str(i) for i in range(5)]\n",
    "\n",
    "\n",
    "def compute_score_on_cv(result, num_splits):\n",
    "    mean_col = 'mean'\n",
    "    splits = split_names(num_splits)\n",
    "    cm_name = 'confusion_matrix'\n",
    "    \n",
    "    assert cm_name in result[mean_col].index\n",
    "    assert len(result[mean_col].loc[cm_name]) == 1\n",
    "    \n",
    "    def compute_scores(col):\n",
    "        cm = np.array(result[col].loc[cm_name][0])\n",
    "        mcm = analytics.confusion_matrix_to_multilabel_confusion_matrix(cm)\n",
    "        scores = analytics.scores_from_multilabel_confusion_matrix(mcm)\n",
    "        \n",
    "        scores['macro_mae'] = compute_mae_from_cm(cm)\n",
    "        scores['macro_kappa'] = compute_kappa_from_cm(cm)\n",
    "        \n",
    "        return scores\n",
    "        \n",
    "    mean = compute_scores(mean_col)\n",
    "    \n",
    "    split_scores = {}\n",
    "    for metric in mean.keys():\n",
    "        split_scores[metric] = []\n",
    "    \n",
    "    for split in splits:\n",
    "        scores = compute_scores(split)\n",
    "        for metric, score in scores.items():\n",
    "            split_scores[metric].append(score)\n",
    "    \n",
    "    split_mean = {}\n",
    "    for metric, scores in split_scores.items():\n",
    "        split_mean[metric] = np.mean(scores)\n",
    "    \n",
    "    return mean, split_mean\n",
    "\n",
    "result_scores = []\n",
    "\n",
    "for _, result in results.iterrows():\n",
    "    approach = result['approach'].split(' ')\n",
    "    infos = approach[1].split('_')\n",
    "       \n",
    "    result_entry = {\n",
    "            'approach_id': approach[0],\n",
    "            'pair': infos[0],\n",
    "            'data': infos[1],\n",
    "            'classes': infos[2],\n",
    "            'approach': infos[3],\n",
    "            'model': infos[4],\n",
    "    }\n",
    "    \n",
    "    id_cols = [c for c in result_entry.keys() if c != 'approach_id']\n",
    "    \n",
    "    mean, split_mean = compute_score_on_cv(result, 5)\n",
    "    \n",
    "    for metric, score in mean.items():\n",
    "        result_entry['mean_' + metric] = score\n",
    "    \n",
    "    \n",
    "    for metric, score in split_mean.items():\n",
    "        result_entry['split_mean_' + metric] = score     \n",
    "    \n",
    "    result_scores.append(result_entry)\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(result_scores)\n",
    "df.sort_values(by=['model', 'approach', 'classes']).drop_duplicates(subset=id_cols).to_csv('/tmp/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1467bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = 'true'\n",
    "normalize = False\n",
    "\n",
    "for _, result in results.iterrows():\n",
    "    mean_result = result['mean']\n",
    "    \n",
    "    if 'confusion_matrix' in mean_result.index:\n",
    "        normalized_cm = []\n",
    "        for cm in mean_result.loc['confusion_matrix']:\n",
    "            if normalize:\n",
    "                cm = analytics.normalize_confusion_matrix(cm, method=normalize)\n",
    "            else:\n",
    "                cm = np.array(cm)\n",
    "\n",
    "        normalized_cm.append(cm)\n",
    "        display(result['approach'])\n",
    "        outcome = analytics.scores_from_confusion_matrices(normalized_cm, mean_result.columns)\n",
    "        display(outcome)\n",
    "        best_f1 = outcome[outcome['macro_f1'] >= outcome['macro_f1'].max()]\n",
    "#         display(best_f1)\n",
    "\n",
    "        plot_shape = (1, 1)\n",
    "        fig_width = 12\n",
    "        if len(normalized_cm) >= plot_shape[0] * plot_shape[1]:\n",
    "            fig_size = (fig_width, int(fig_width / plot_shape[1] * plot_shape[0] * 0.85))\n",
    "#             analytics.plot_epochs_confution_matrix(result['approach'], normalized_cm, mean_result.columns, plot_shape=plot_shape, figsize=fig_size)\n",
    "\n",
    "#         analytics.plot_reg(np.array(mean_result.loc['confusion_matrix'][best_f1.index[0]]))\n",
    "        \n",
    "    elif 'multilabel_confusion_matrix' in mean_result.index:\n",
    "        for mcm in mean_result.loc['multilabel_confusion_matrix']:\n",
    "#             for cm in mcm:\n",
    "#                 disp = ConfusionMatrixDisplay(np.array(cm))\n",
    "#                 disp.plot()\n",
    "#                 pyplot.show()\n",
    "            mcm = np.array(mcm)\n",
    "            display(mcm)\n",
    "            display(analytics.scores_from_multilabel_confusion_matrix(mcm))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
