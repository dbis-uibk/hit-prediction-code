{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hit_prediction_code.analytics import get_results_as_dataframe\n",
    "from hit_prediction_code import analytics\n",
    "\n",
    "results = get_results_as_dataframe(\n",
    "    project_name='hit-prediction-ismir2020',\n",
    "    table_name='hit_prediction',\n",
    "    filter_git_dirty=True,\n",
    "    date_filter='> \\'2021-08-04 16:20:00\\'',\n",
    "    columns=['id', 'sourcefile', 'outcome'],\n",
    "#     filters=['sourcefile LIKE \\'plans/hspd/%%.py\\''],\n",
    "    filters=['sourcefile LIKE \\'plans/hspd/binary_class_%%_melspect%%fcn%%.py\\''],\n",
    ")\n",
    "\n",
    "analytics.add_approach_to_df(results)\n",
    "analytics.add_cv_epoch_evaluator_outcome_to_df(results)\n",
    "\n",
    "results.sort_values(by='sourcefile', inplace=True)\n",
    "display(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(cm):\n",
    "    def cm_acc(matrix):\n",
    "        tn, fp, fn, tp = np.array(matrix).ravel()\n",
    "        \n",
    "        return (tn + tp) / (tn + fp + fn + tp)\n",
    "    \n",
    "    acc = 0\n",
    "    for m in cm:\n",
    "        acc += cm_acc(m)\n",
    "    \n",
    "    return acc / len(cm)\n",
    "\n",
    "def compute_f1(cm):\n",
    "    def cm_f1(matrix):\n",
    "        tn, fp, fn, tp = np.array(matrix).ravel()\n",
    "        \n",
    "        return (2 * tp) / (2 * tp + fp + fn)\n",
    "    \n",
    "    f1 = 0\n",
    "    for m in cm:\n",
    "        f1 += cm_f1(m)\n",
    "    \n",
    "    return f1 / len(cm)\n",
    "\n",
    "for _, row in results.iterrows():\n",
    "    acc = row['mean'].loc['multilabel_confusion_matrix'].apply(compute_acc)\n",
    "    f1 = row['mean'].loc['multilabel_confusion_matrix'].apply(compute_f1)\n",
    "#     print(row['id'], row['sourcefile'], acc)\n",
    "    print(row['id'], row['sourcefile'], f'acc: {acc.max().round(3)}', f'f1: {f1.max().round(3)}')\n",
    "#     print(row['mean'].max(axis=1))\n",
    "#     print(row['mean'].loc['multilabel_confusion_matrix'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['final_outcome'] = results['mean'].apply(lambda v: v.tail())\n",
    "# results['best_outcome'] = results['mean'].apply(lambda v: v.max(axis=1))\n",
    "\n",
    "def display_outcome():\n",
    "    metrics = ['f1_macro', 'precision_macro', 'recall_macro']\n",
    "    outcome = []\n",
    "    for _, row in results.iterrows():\n",
    "        out = {}\n",
    "        out['id'] = row['id']\n",
    "        best = row['mean'].max(axis=1)\n",
    "        for m in metrics:\n",
    "            out[m] = best[m]\n",
    "        outcome.append(out)\n",
    "\n",
    "    outcome = pd.DataFrame(outcome).merge(results[['id', 'sourcefile']], on=['id'])\n",
    "    outcome['name'] = outcome['id'].apply(str) + ' ' + outcome['sourcefile']\n",
    "    outcome[['name'] + metrics].plot.bar(x='name', title='best', figsize=(24,6))\n",
    "    plt.show()\n",
    "\n",
    "results = results.sort_values(by=['sourcefile'])\n",
    "# display(results[['id', 'sourcefile', 'best_epoch']])\n",
    "\n",
    "display_outcome()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
